{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to YelpHelp! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The purpose of this notebook is to build a model that classifies whether a Yelp review comes from a stable period in a restaurant's history or from a \"tipping point\" after which there will be a decline in user satisfaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code is organized into a number of different sections: <br> <br> First I'll load in the module I wrote to process these reviews as well as other useful modules, and then we'll create connections to the SQL databases for the businesses and reviews. <br> <br> Second I'll do data exploration to learn something about tipping point reviews. <br><br> Third, I'll process all of the reviews to generate features for the model and then split it into training and testing sets. <br> <br> Finally, I'll train the model and then compare it to other possible approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Set up approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load from the yelphelp package I wrote to get functions for querying our databases,\n",
    "#processing the reviews to derive features, building the bag of words representation\n",
    "#and visualizing the results\n",
    "from yelphelp.queries import query_business_switchpoints, query_business_reviews\n",
    "from yelphelp.pp_switchpoints import mcmc_changepoint\n",
    "from yelphelp.data_prep import get_business_class, sample_reviews, scale_features\n",
    "from yelphelp.nlp_tools import create_nlp_features, get_pos_dist, service_complaint\n",
    "from yelphelp.viz_tools import make_roc\n",
    "\n",
    "#SQL dependencies\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "#I/O\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#Analysis/Data organization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from scipy import interpolate\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#NLP & Feature Analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Visualization/Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Probabilistic Programming\n",
    "import pymc3 as pm\n",
    "from pymc3 import Model, DiscreteUniform, Exponential, Poisson, Normal \n",
    "from pymc3 import HalfNormal, NUTS, Metropolis, sample, traceplot, find_MAP\n",
    "from pymc3.math import switch\n",
    "\n",
    "#Modeling\n",
    "import xgboost as xgb \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#Saving results\n",
    "import pickle\n",
    "\n",
    "#Visualization\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connections to the SQL databases we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three relevant databases: \"Reviews\" which contains ~4M individual reviews, \"Businesses\" which contains information about each unique business discussed in the reviews, and \"Switchpoints\" which lists the review number corresponding to a business' most likely switchpoint as well as the magnitude of the change after the switchpoint. <br><br> I then query the \"businesses\" database to get a list of the businesses that will be included in the model. I decided to include only (a) restaurants that have (b) at least 40 reviews and (c) are West of the -60 meridian (this is to exclude a small number of German restaurants that show up in the set and would not be accurately handled by the NLP scripts) <br><br>--An important note on \"Switchpoints\": this database was created by running the code in Find Tipping Points (the MCMC takes about ~15s per business and so I have opted to separate the code so you don't have to wait days to play with the model).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define the names of the 3 databases I created\n",
    "reviews_database = 'reviews'\n",
    "business_database = 'businesses'\n",
    "switch_database = 'switchpoints'\n",
    "\n",
    "username = 'steven' ##This user name must match what's set up in PostgreSQL!\n",
    "\n",
    "#Establish connections to each of the databases\n",
    "business_con = None\n",
    "business_con = psycopg2.connect(database = business_database, user = username)\n",
    "review_con = None\n",
    "review_con = psycopg2.connect(database = reviews_database, user = username)\n",
    "switch_con = None\n",
    "switch_con = psycopg2.connect(database = switch_database, user = username)\n",
    "\n",
    "#Here I run a query to get the businesses to include in the modeling. This query only has to be run once\n",
    "#and creates the Pandas dataframe \"business_data_from_sql\" that I will use to know which reviews to pull\n",
    "#from the reviews database.\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM business_data_table WHERE categories LIKE '%Restaurants%' AND longitude < -60 AND review_count > 40;\n",
    "\"\"\"\n",
    "\n",
    "business_data_from_sql = pd.read_sql_query(sql_query,business_con)\n",
    "\n",
    "print('There are ' + str(len(business_data_from_sql)) + ' restaurants reviewed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) What does a tipping point look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Query the reviews database to get all of the reviews associated with the 30th business_id in the business database\n",
    "business_reviews = query_business_reviews(business_data_from_sql['business_id'].iloc[30],review_con)\n",
    "\n",
    "#This line runs the probabilistic programming function I wrote to discover & characterize the tipping point\n",
    "#in a restaurant that has a pretty good example of what I'm trying to find\n",
    "difference, a_switchpoint, switch_p, d,e = mcmc_changepoint(business_reviews['date'],\n",
    "            business_reviews['user_normed_stars'],mcmc_iter=10000,plot_result=1)\n",
    "\n",
    "#In the trace plot below, the most intersting thing is the upper left panel \"switchpoint\" which shows the\n",
    "#histogram of where the MCMC algorithm put the switch from one normal disribution to the other. You can see that\n",
    "#it's pretty constrained between the 20th and 40th review. The two plots below that show the estimated\n",
    "#distributions for the mean of the before and after switchpoint normals. The way to read this is that before the\n",
    "#switch point, the most likely mean review is around .25--or slightly above average--but after the switchpoint\n",
    "#the mean review is around -.75, meaning a drop of about a full star. \n",
    "\n",
    "#Finally, at the very bottom you can see a plot of the review history. Each circle is one review, and the line\n",
    "#is the estimate from the probabilistic programming (i.e. each iteration of the function finds a step function\n",
    "#and the line is the average of all of those step functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) How frequent are tipping points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First, I query the switchpoint database to find wall of the businesses \n",
    "#that have large changes after a \"tipping point.\"\n",
    "switches_df = query_business_switchpoints(switch_con)\n",
    "\n",
    "#Differential stores the values of the change (before tipping-after tipping). \n",
    "#Here I make sure that the values are stored in the rigth format (i.e. as float)\n",
    "t_busi_changes = switches_df['Differential'].values\n",
    "busi_changes = [float(x) for x in t_busi_changes]\n",
    "busi_changes = np.array(busi_changes)\n",
    "\n",
    "print('How frequent are large tipping points?')\n",
    "\n",
    "#Create a histogram showing the magnitude of tipping points. (I multiply by -1 so losses are on the left)\n",
    "p, bins, patches = plt.hist(-1*np.array(busi_changes), bins=100,color= (.1,.2,.8))\n",
    "\n",
    "#I then color anything that is has a \"large\" tipping point (3/4ths a star) as red\n",
    "\n",
    "#(Note I put the visualization threshold at -.8 rather than the -.75 cutoff I actually use\n",
    "#This is because if I choose -.75 it aliases the graph. This means that my visualization is\n",
    "#in theory a little conservative--i.e. it shows a handful fewer than there really are. However,\n",
    "#in practice this doesn't matter because I'm going to draw a black line that obscures this part\n",
    "#of the plot--I just don't want there to be confusing mixes of colors around it).\n",
    "for patchnum in range(len(patches)):\n",
    "    if bins[patchnum] <= -.8: \n",
    "        patches[patchnum].set_facecolor((.7,.1,.1))\n",
    "\n",
    "#Draw a black line to indicate where our cut off is\n",
    "p = plt.plot([-.75,-.75],[0,500],'k')\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "ax.tick_params('both',color='black')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "plt.show()\n",
    "\n",
    "#Save the x-axis limits so we can make other visualizations parallel to it \n",
    "hist_lims = ax.get_xlim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Are tipping point restaurants less likely to stay open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For my third visualization I wanted to see whether these tipping points matter-- afterall, it's an\n",
    "#exploratory idea of mine that they exist at all, and although they're prominent in the above histogram\n",
    "#it has a normal distribution which could just indicate that they're noise rather than a distinct subset.\n",
    "#The one encouraging part is that the DECREASES are far more likely to occur than INCREASES, suggesting \n",
    "#there might be something real going on.\n",
    "\n",
    "#If tipping points are real, then they should hurt a business, and we might expect restaurants with \n",
    "#tipping points to be less likely to stay open. \n",
    "\n",
    "thresh = []\n",
    "closed_p = []\n",
    "\n",
    "#In this for loop I want to get the probability that restaurants are open based on the magnitude of their\n",
    "#tipping points. To do this I'm going to take averages within a window centered on every possible magnitude\n",
    "#of tipping point based on the range we saw above.\n",
    "\n",
    "delta = .2 #This is the bin or window width: so if the tipping threshold is .1 then the bin is -.1:.3\n",
    "s_threshes = np.arange(-3,3,.01)\n",
    "for this_thresh in s_threshes:\n",
    "    #Get the indicies where we're within the bin\n",
    "    declines_ind = np.where(\n",
    "        np.logical_and(busi_changes >= this_thresh-delta, \n",
    "                       busi_changes<=this_thresh+delta))[0] \n",
    "    #Now pull from the business database whether those buisnesses are open or closed\n",
    "    closed = []\n",
    "    for an_index in np.arange(0,len(declines_ind)):\n",
    "        closed.append(business_data_from_sql['is_open'][declines_ind[an_index]] )\n",
    "\n",
    "    #Record the center of this bin (the switchpoint threshold, and calculate the proportion of open places)\n",
    "    thresh.append(this_thresh)\n",
    "    closed_p.append(np.mean(closed))\n",
    "\n",
    "    \n",
    "#One problem with the above is that it's very very noisy, and depends pretty heavily on the bandwidth (delta)\n",
    "#For visualization's sake, I'm going to apply locally weighted linea regression to create an easy-to-read \n",
    "#curve out of the points (x:threshold, y:probability) I created in the last step\n",
    "import statsmodels.api as sm\n",
    "lowess = sm.nonparametric.lowess\n",
    "\n",
    "lowessline = lowess(closed_p, thresh,frac=.5)\n",
    "\n",
    "thresh = np.array(thresh)\n",
    "lowessline = np.array(lowessline)\n",
    "\n",
    "#Plot the function we estimated:\n",
    "plt.plot(-1*thresh,lowessline[:,1]*100,color= (.1,.2,.8),linewidth=4) \n",
    "\n",
    "#Now color the \"tipping points\" below our threshold in red:\n",
    "inds = np.where(-1*thresh < -.75)\n",
    "plt.plot(-1*thresh[inds],lowessline[inds,1].flatten()*100,color= (.7,.1,.1),linewidth=4) \n",
    "\n",
    "#Plot a black line to show our definition of tipping points\n",
    "ax = plt.gca()\n",
    "linestart = ax.get_ylim()\n",
    "plt.plot([-.75,-.75],[70,100],'k')\n",
    "\n",
    "ax.set_facecolor('white')\n",
    "ax.tick_params('both',color='black')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.set_xlim(hist_lims)\n",
    "plt.ylim([70,100])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Construct the data set & perform feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this section I pull reviews from tipping points and stable periods. I then split these into training and testing sets and extract the features to enter into the model. At the end of this section I will have: <br><br> (1) A bag of words representation of each review in the set <br><br> (2) A part-of-speech distribution (i.e. how many nouns, verbs, etc) for each review <br><br> (3) A sentiment analysis of each review as well as the sentiment of words occuring near mentions of the \"server\" <br><br> (4) Simple statistics of each review (e.g. number of words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "#NOTE: this cell takes a while to run\n",
    "#############################################################################################################\n",
    "\n",
    "#What we need to do next is actually create a dataset of reviews that come from tipping points and reviews\n",
    "#that come from stable periods. So far we've just been looking at businesses that have or do not have these\n",
    "#tipping points.\n",
    "\n",
    "#Define some important parameters for what to include in the dataset\n",
    "\n",
    "#How many reviews do I want to sample from each business?\n",
    "num_samples = 10 #I use ceil, so it actually can be num_samples+1\n",
    "\n",
    "#What's the minumum number of reviews allowed (i.e. we don't want the first review to be a tipping point)\n",
    "minimum_switch = 15 \n",
    "    \n",
    "#Query the switchpoint dataframe (in case we skipped the visualization cells)\n",
    "switches_df = query_business_switchpoints(switch_con)\n",
    "\n",
    "business_trajectory = [] #This will be our dependent variable\n",
    "all_reviews = [] #This will be a list of all the pre-processed review text\n",
    "tipping_location = [] #Record when in the business' run the tipping point occurs\n",
    "numeric_features = np.empty( (14,) ) #This will hold the features we engineer within the sample_review function\n",
    "\n",
    "busi_count = 0\n",
    "print(len(switches_df['business_id'])) \n",
    "switch_df_ids = switches_df['business_id']\n",
    "for busi_ind in switches_df['business_id']: #6933\n",
    "\n",
    "    #Get the review at which the switchpoint occured for this busines id\n",
    "    business_switchpoint = switches_df['SwitchPoint'].loc[ switches_df['business_id'] == busi_ind ].values[0]\n",
    "    business_switchpoint = int(business_switchpoint)\n",
    "    #Get the size of that switchpoint\n",
    "    business_differential = switches_df['Differential'].loc[ switches_df['business_id'] == busi_ind ].values[0]\n",
    "    business_differential = float(business_differential)\n",
    "    \n",
    "    #Determine the label we'd give to reviews around the switchpoint: stable or tipping\n",
    "    #If the change after the switchpoint is large, label it tipping; else label it stable\n",
    "    business_classification = get_business_class(business_differential)\n",
    "    \n",
    "    if business_switchpoint > minimum_switch:\n",
    "    \n",
    "        busi_count += 1\n",
    "        print(busi_count)\n",
    "            \n",
    "        #Get all reviews for this business\n",
    "        business_reviews = query_business_reviews(busi_ind,review_con)\n",
    "        \n",
    "        #Get the N reviews around the switchpoint, preprocess (i.e. lemmatize) them, and derive simple statistics\n",
    "        review_samples, review_features = sample_reviews(business_reviews,business_switchpoint,num_samples)\n",
    "                \n",
    "        print('Business #' + str(busi_ind) + ' had ' \n",
    "              + str(len(review_samples)) + ' reviews (switch: ' \n",
    "              + str(business_switchpoint) + ' length: ' + str(len(business_reviews)) + ')')\n",
    "\n",
    "        if len(review_samples)>0:\n",
    "            #Append this to tipping locations ONLY if there's a real tipping point\n",
    "            if business_classification == 1:\n",
    "                tipping_location.append(business_switchpoint/len(business_reviews))\n",
    "            \n",
    "            #Add all of these to our lists\n",
    "            for a_review_sample in review_samples:\n",
    "                business_trajectory.append(business_classification)\n",
    "                all_reviews.append(a_review_sample)\n",
    "\n",
    "            numeric_features = np.vstack([numeric_features,review_features])\n",
    "\n",
    "#Because we began the numeric features as an empty Numpy array, we have an extra empty row at the beginning\n",
    "numeric_features = numeric_features[1:]\n",
    "\n",
    "#This code is to show where tipping points are most likely to occur during a restaurant's run.\n",
    "#It makes clear that tipping points are more likely towards the end.\n",
    "plt.hist(tipping_location,bins=100,color='cyan')\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's get a count of how many reviews we had to consider to get find these tipping points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "how_many = 0\n",
    "for busi_ind in switches_df['business_id']:\n",
    "    how_many += business_data_from_sql['review_count'].loc[business_data_from_sql['business_id']==busi_ind].values\n",
    "    \n",
    "print('We considered ' + str(how_many[0]) + ' reviews from ' \n",
    "      + str(len(switches_df['business_id'])) + ' different businesses.' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#At this point, split into Train and Test sets. \n",
    "\n",
    "#(Note, I don't split into an explicit validation set here because if I need to I just split the train set)\n",
    "decline_ind = np.where(np.array(business_trajectory)==1)[0]\n",
    "stable_ind = np.where(np.array(business_trajectory)==0)[0]\n",
    "incline_ind = np.where(np.array(business_trajectory)==-1)[0]\n",
    "\n",
    "############################################################################################\n",
    "#First subsample the data so there are even numbers of stable period & tipping point reviews\n",
    "############################################################################################\n",
    "\n",
    "#Randomly shuffle the indicies\n",
    "np.random.seed(seed=12345678)\n",
    "np.random.shuffle(decline_ind)\n",
    "np.random.shuffle(stable_ind)\n",
    "np.random.shuffle(incline_ind)\n",
    "\n",
    "#Now subselect stable indicies to match the number of tipping points\n",
    "stable_ind = stable_ind[np.arange(0,len(decline_ind)+1)]\n",
    "\n",
    "############################################################################################\n",
    "#Second, split into a train set with 66% of reviews and a test set with 34%\n",
    "############################################################################################\n",
    "\n",
    "#Cut both stable and tipping inds into training and testing sets\n",
    "train_length = int(np.floor(len(decline_ind) * .66))\n",
    "test_endpoint = int(len(decline_ind))\n",
    "\n",
    "#Train set indicies\n",
    "stable_train_ind = stable_ind[np.arange(0,train_length)]\n",
    "decline_train_ind = decline_ind[np.arange(0,train_length)]\n",
    "\n",
    "#Test set indicies\n",
    "stable_test_ind = stable_ind[np.arange(train_length+1, test_endpoint)]\n",
    "decline_test_ind = decline_ind[np.arange(train_length+1, test_endpoint)]\n",
    "\n",
    "############################################################################################\n",
    "#Third, merge the train stable and train decline indicies together to form a single list of\n",
    "#training set indicies. Then do the same procedure for the testing set indicies\n",
    "############################################################################################\n",
    "\n",
    "train_ind = []\n",
    "train_ind.append(stable_train_ind)\n",
    "train_ind.append(decline_train_ind)\n",
    "test_ind = []\n",
    "test_ind.append(stable_test_ind)\n",
    "test_ind.append(decline_test_ind)\n",
    "\n",
    "train_ind = np.array(train_ind).flatten()\n",
    "test_ind = np.array(test_ind).flatten()\n",
    "\n",
    "#Now shuffle these so they're not sorted\n",
    "np.random.shuffle(train_ind)\n",
    "np.random.shuffle(test_ind)\n",
    "\n",
    "############################################################################################\n",
    "#Finally, apply these indicies to divide the features/labels into training and testing sets\n",
    "############################################################################################\n",
    "all_reviews = np.array(all_reviews)\n",
    "business_trajectory = np.array(business_trajectory)\n",
    "numeric_features = np.array(numeric_features)\n",
    "\n",
    "train_reviews = all_reviews[train_ind]\n",
    "train_numeric_features = numeric_features[train_ind]\n",
    "train_trajectory = business_trajectory[train_ind]\n",
    "\n",
    "test_reviews = all_reviews[test_ind]\n",
    "test_numeric_features = numeric_features[test_ind]\n",
    "test_trajectory = business_trajectory[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bag of words representation and run other NLP analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we've split the reviews into the training and testing sets, we can get our bag of words for each.\n",
    "#The important part is that we have to build our vocabulary--and pickle our vectorizer--on the training set\n",
    "tfid_train_reviews, nlp_feature_names = create_nlp_features(train_reviews,is_training=True)\n",
    "#We can then apply the exact same vectorizer to the testing set. \n",
    "tfid_test_reviews, nlp_feature_names = create_nlp_features(test_reviews,is_training=False)\n",
    "\n",
    "#Create a list of features corresponding to the parts of speech within each review\n",
    "train_pos_dist = []\n",
    "for review in train_reviews:\n",
    "    train_pos_dist.append(get_pos_dist(review))\n",
    "\n",
    "test_pos_dist = []\n",
    "for review in test_reviews:\n",
    "    test_pos_dist.append(get_pos_dist(review))\n",
    "\n",
    "#Now do the same for the sentiment around any mention of the server\n",
    "train_service_mentions = []\n",
    "for review in train_reviews:\n",
    "    train_service_mentions.append(service_complaint(review))\n",
    "\n",
    "test_service_mentions = []\n",
    "for review in test_reviews:\n",
    "    test_service_mentions.append(service_complaint(review))\n",
    "\n",
    "#Compile the numeric feature names\n",
    "continuous_feature_names = ['negative', 'neutral', 'positive', 'composite', \n",
    "                            'positive to negative ratio', 'service sentiment', \n",
    "                            'rating', 'normed_rating', 'useful', 'num_words', \n",
    "                            'current mean rating', 'slope', 'rating standard deviation', \n",
    "                            'rating ratio', 'nouns', 'adjectives','adverbs','verbs', \n",
    "                            'adjectives:nouns', 'adverb:verb', 'verb:noun', 'server_mentions', \n",
    "                            'wait_mentions', 'sick_mentions', 'chef_mentions', 'empty_mentions', \n",
    "                            'ill_mentions', 'temperature_mentions', 'change_mentions', 'switch_mentions', \n",
    "                            'expensive_mentions', 'forgot_mentions','service_mentions']\n",
    "\n",
    "all_feature_names = []\n",
    "all_feature_names.extend(nlp_feature_names)\n",
    "all_feature_names.extend(continuous_feature_names)\n",
    "\n",
    "#Stack all the engineered features together and rescale them\n",
    "train_continuous_features = np.hstack( \n",
    "    [train_numeric_features, np.array(train_pos_dist), np.array(train_service_mentions)] )\n",
    "test_continuous_features = np.hstack( \n",
    "    [test_numeric_features, np.array(test_pos_dist), np.array(test_service_mentions)] )\n",
    "\n",
    "scaled_train_features = scale_features(train_continuous_features,is_training=True)\n",
    "scaled_test_features = scale_features(test_continuous_features,is_training=False)\n",
    "\n",
    "train_data = sparse.hstack( [tfid_train_reviews, scaled_train_features] ).tocsc()\n",
    "test_data = sparse.hstack( [tfid_test_reviews, scaled_test_features] ).tocsc()\n",
    "\n",
    "train_X = train_data\n",
    "train_y = train_trajectory\n",
    "test_X = test_data\n",
    "test_y = test_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everyone loves wordclouds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now that we've collected all of the specific reviews we're going to use in our modeling problem, it's good\n",
    "#to just take a look at the most frequently used words to see if anything pops out that might be useful a prior.\n",
    "\n",
    "#I prefer to look *only* at the training set--I recognize it's important to ensure that the training and testing\n",
    "#sets are from the same underlying distributions, however, this is true by construction because I pull them \n",
    "#randomly from the same set of businesses. \n",
    "\n",
    "#I am more wary of basing my modeling around something idiosyncrating to the particular training-testing split. \n",
    "#And, in this case it doesn't matter because it's clear that absolute word frequency alone won't be enough.\n",
    "\n",
    "#I want to create spearate wordclouds for the stable and tipping point reviews\n",
    "s_ind = np.where(np.array(train_trajectory)==0)\n",
    "t_ind = np.where(np.array(train_trajectory)==1)\n",
    "\n",
    "s_words = np.array(train_reviews)[s_ind]\n",
    "t_words = np.array(train_reviews)[t_ind]\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "    \n",
    "###Add Matplotlib colormaps\n",
    "wc_color_map = 'plasma'\n",
    "\n",
    "#Create wordclouds without adding in stopwords to show how difficult this is\n",
    "print('If we just look at the words present....')\n",
    "print('Stable periods')\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopword_set,colormap = wc_color_map, background_color='white').generate(' '.join(s_words))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('Tipping points')\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stopword_set,colormap = wc_color_map, background_color='white').generate(' '.join(t_words))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression (this is what I ultimately used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here I build a logistic regression with elastic net regularization\n",
    "model = SGDClassifier(loss='log',class_weight='balanced', penalty='elasticnet',n_iter=100) #alpha=0.0001, l1_ratio=0.15, n_iter=100)\n",
    "\n",
    "#Grid search to find the ideal hyperparameters for the elastic net\n",
    "hyper_parameters = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05], \n",
    "                     'l1_ratio': [0.05, .15, .25, .35, .45, .55, .65, .75, .85, .95]},\n",
    "cv_model = model_selection.GridSearchCV(model, hyper_parameters, cv=5)\n",
    "cv_model.fit(train_X,train_y)\n",
    "\n",
    "best_alpha = cv_model.best_estimator_.alpha\n",
    "best_l1_ratio = cv_model.best_estimator_.l1_ratio\n",
    " \n",
    "print('CV grid search param performance:' + str(cv_model.best_score_) + ' ' + str(best_alpha)+ ' ' + str(best_l1_ratio))\n",
    "\n",
    "#Construct a model with those parameters\n",
    "model = SGDClassifier(loss='log',class_weight='balanced', penalty='elasticnet',n_iter=100,alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "#Save the model so we can port it to AWS & power the website\n",
    "saved_weights = pickle.dump(model, open( \"logistic_weights\", \"wb\" ) )\n",
    "\n",
    "#Calculate performance\n",
    "performance = model.score(test_X,test_y)\n",
    "\n",
    "#Save the coefficients to invesigate feature importance\n",
    "coefs = model.coef_ \n",
    "\n",
    "#Create an ROC plot\n",
    "switch_probabilities = model.predict_proba(test_X)\n",
    "make_roc(test_y,switch_probabilities[:,1],plot_me=1)\n",
    "    \n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=10 #How many features do we want to look at?\n",
    "\n",
    "coefs = coefs.flatten()\n",
    "coef_inds = coefs.argsort()\n",
    "\n",
    "plt.barh(np.arange(N),np.abs(coefs[coef_inds[:N]]), color = (.7,.1,.1) )\n",
    "all_feature_names = np.array(all_feature_names)\n",
    "print(coefs[coef_inds[:N]])\n",
    "print(all_feature_names[coef_inds[:N]])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(np.arange(N),np.flip(np.abs(coefs[coef_inds[-N:]]),axis=0), color = (.1,.2,.8))\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "print(np.flip(all_feature_names[coef_inds[-N:]],axis=0))\n",
    "print(coefs[coef_inds[-N:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['axes.facecolor']='white'\n",
    "\n",
    "model = MultinomialNB(alpha=15)\n",
    "\n",
    "#Grid search to get the best smoothing parameter\n",
    "hyper_parameters = {'alpha': [0.1, 0.5, 1, 5, 10, 15, 30]} \n",
    "cv_model = model_selection.GridSearchCV(model, hyper_parameters, cv=5)\n",
    "cv_model.fit(tfid_train_reviews,train_y)\n",
    "\n",
    "best_alpha = cv_model.best_estimator_.alpha\n",
    "model = MultinomialNB(alpha=best_alpha)\n",
    "\n",
    "model.fit(tfid_train_reviews, train_y)\n",
    "switch_probabilities = model.predict_proba(tfid_test_reviews)\n",
    "make_roc(test_y,switch_probabilities[:,1],plot_me=1)\n",
    "\n",
    "print(tfid_train_reviews.shape)\n",
    "print(tfid_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function for passing data to XGBoost\n",
    "def train_XGB_model(train_data, train_labels, test_data, test_labels=None, seed=0, iterations=1000):\n",
    "    \"\"\"Function for training or cross-validating an XGBoost model\"\"\"\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob' #if doing reg: 'reg:linear'#if doing class: 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 4\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = len(set(train_labels))\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed\n",
    "    #Add regularization parameters\n",
    "    #param['lambda'] = .05 #Ridge; L2\n",
    "    #param['alpha'] = .01 #LASSO; L1\n",
    "\n",
    "\n",
    "    parameter_list = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_data, label=train_labels)\n",
    "    \n",
    "    if test_labels is not None: #i.e. we're doing cross-validation\n",
    "        xgtest = xgb.DMatrix(test_data, label=test_labels)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest,'test') ]\n",
    "        model = xgb.train(parameter_list, xgtrain, iterations, watchlist, early_stopping_rounds=30)\n",
    "    else: #we're making real predictions \n",
    "        xgtest = xgb.DMatrix(test_data)\n",
    "        model = xgb.train(parameter_list, xgtrain, iterations)\n",
    "    \n",
    "    predicted_labels = model.predict(xgtest)\n",
    "    return predicted_labels, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the XGBoost\n",
    "predictions, model = train_XGB_model(train_X,train_y,test_X,test_labels=None)\n",
    "\n",
    "#Calculate its accuracy on the testing set\n",
    "guesses = list(map(lambda x: np.argmax(x),predictions))\n",
    "print(np.sum(guesses==test_y) / len(guesses))\n",
    "\n",
    "#Plot its ROC curve\n",
    "make_roc(test_y,predictions[:,1],plot_me=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
